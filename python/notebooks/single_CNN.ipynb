{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training od single CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siamese.dataset import DataLoader\n",
    "from siamese.dataset import Dataset\n",
    "from extensies import augmentation as a\n",
    "from siamese.model import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data from mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "Number of images in x_train 60000\n",
      "Number of images in x_test 10000\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "x_train = x_train.reshape((60000,28,28,1))\n",
    "x_test = x_test.reshape((10000,28,28,1))\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "print('Number of images in x_test', x_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data  from our PROSTATEx datast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "modalities = ['adc/t/30x30x1']\n",
    "if len(modalities) > 1:\n",
    "    loader = DataLoader('../../data/',modalities)\n",
    "    loader.load_data()\n",
    "    loader.combine_channels(modalities)\n",
    "    x_train, x_test, y_train, y_test = loader.get_train_test('combined')\n",
    "\n",
    "else:\n",
    "    loader = DataLoader('../../data/',modalities)\n",
    "    loader.load_data()\n",
    "    x_train, x_test, y_train, y_test = loader.get_train_test(modalities[0])\n",
    "    \n",
    "dataset = Dataset()\n",
    "dataset.images_train = x_train\n",
    "dataset.images_test = x_test\n",
    "dataset.labels_train = y_train\n",
    "dataset.labels_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = dataset.get_batch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 30, 30, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting of model and optimalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:  Tensor(\"model/fc3/fc3/Relu:0\", shape=(?, 2), dtype=float32)\n",
      "loss:  Tensor(\"loss:0\", shape=(), dtype=float32)\n",
      "predicted_labels:  Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = xmas_model\n",
    "placeholder_shape = [None] + list(x_train.shape[1:])\n",
    "x = tf.placeholder(tf.float32, placeholder_shape, name='x')\n",
    "y = tf.placeholder(dtype = tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "net_output = model(x, reuse=False)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = net_output, name = 'softmax'),name = 'loss')\n",
    " \n",
    "\n",
    "correct_pred = tf.argmax(net_output, 1)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(correct_pred,y),tf.float32))\n",
    "\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "train_step = tf.train.AdamOptimizer(0.00001).minimize(loss, global_step = global_step)\n",
    "\n",
    "print(\"logits: \", net_output)\n",
    "print(\"loss: \", loss)\n",
    "print(\"predicted_labels: \", acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Loss= 0.693139, Loss test= 0.693147, Acc= 0.752577, AccA= 0.746032\n",
      "Iter 20, Loss= 0.684122, Loss test= 0.679791, Acc= 0.752577, AccA= 0.698413\n",
      "Iter 40, Loss= 0.660988, Loss test= 0.660027, Acc= 0.752577, AccA= 0.777778\n",
      "Iter 60, Loss= 0.622020, Loss test= 0.630170, Acc= 0.752577, AccA= 0.801587\n",
      "Iter 80, Loss= 0.589200, Loss test= 0.590325, Acc= 0.752577, AccA= 0.753968\n",
      "Iter 100, Loss= 0.560557, Loss test= 0.554637, Acc= 0.752577, AccA= 0.738095\n",
      "Iter 120, Loss= 0.505978, Loss test= 0.541736, Acc= 0.752577, AccA= 0.785714\n",
      "Iter 140, Loss= 0.592188, Loss test= 0.542355, Acc= 0.752577, AccA= 0.722222\n",
      "Iter 160, Loss= 0.545471, Loss test= 0.541158, Acc= 0.752577, AccA= 0.753968\n",
      "Iter 180, Loss= 0.496927, Loss test= 0.540481, Acc= 0.752577, AccA= 0.793651\n",
      "Iter 200, Loss= 0.567344, Loss test= 0.540235, Acc= 0.752577, AccA= 0.722222\n",
      "Iter 220, Loss= 0.542068, Loss test= 0.540924, Acc= 0.752577, AccA= 0.753968\n",
      "Iter 240, Loss= 0.516450, Loss test= 0.539155, Acc= 0.752577, AccA= 0.777778\n",
      "Iter 260, Loss= 0.553783, Loss test= 0.538992, Acc= 0.752577, AccA= 0.722222\n",
      "Iter 280, Loss= 0.497287, Loss test= 0.538504, Acc= 0.752577, AccA= 0.777778\n",
      "Iter 300, Loss= 0.470922, Loss test= 0.538724, Acc= 0.752577, AccA= 0.801587\n",
      "Iter 320, Loss= 0.480269, Loss test= 0.537195, Acc= 0.752577, AccA= 0.793651\n",
      "Iter 340, Loss= 0.552862, Loss test= 0.538369, Acc= 0.752577, AccA= 0.730159\n",
      "Iter 360, Loss= 0.450228, Loss test= 0.536765, Acc= 0.752577, AccA= 0.817460\n",
      "Iter 380, Loss= 0.470803, Loss test= 0.535462, Acc= 0.752577, AccA= 0.793651\n",
      "Iter 400, Loss= 0.442559, Loss test= 0.537340, Acc= 0.752577, AccA= 0.809524\n",
      "Iter 420, Loss= 0.484516, Loss test= 0.534964, Acc= 0.752577, AccA= 0.769841\n",
      "Iter 440, Loss= 0.503391, Loss test= 0.533646, Acc= 0.752577, AccA= 0.753968\n",
      "Iter 460, Loss= 0.459453, Loss test= 0.533211, Acc= 0.752577, AccA= 0.761905\n",
      "Iter 480, Loss= 0.460787, Loss test= 0.533507, Acc= 0.752577, AccA= 0.785714\n",
      "Iter 500, Loss= 0.502751, Loss test= 0.535091, Acc= 0.752577, AccA= 0.714286\n",
      "Iter 520, Loss= 0.440670, Loss test= 0.538002, Acc= 0.752577, AccA= 0.769841\n",
      "[0 0 0 ... 0 0 0]\n",
      "Accuracy: 0.7525773195876289\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: 0.0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 126\n",
    "aug = False\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #setup tensorboard\t\n",
    "    tf.summary.scalar('step', global_step)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    for var in tf.trainable_variables():\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    #train iter\n",
    "    for i in range(540):\n",
    "\n",
    "        batch_x, batch_y = dataset.get_batch(batch_size,aug)\n",
    "        # Run optimization op (backprop)x.\n",
    "            # Calculate batch loss and accuracy\n",
    "        loss_a,opt,acc_a = sess.run([loss, train_step, acc], feed_dict={x: batch_x, y: batch_y})\n",
    "        \n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            loss_b,acc_b = sess.run([loss,acc], feed_dict={x: x_test, y: y_test})\n",
    "        \n",
    "            print(\"Iter \" + str(i) + \", Loss= \" + \\\n",
    "                      \"{:.6f}\".format(loss_a) + \", Loss test= {:.6f}\".format(loss_b)+ \", Acc= {:.6f}\".format(acc_b)+ \", AccA= {:.6f}\".format(acc_a))\n",
    "            \n",
    "    pred = sess.run(correct_pred, feed_dict={x: x_test})\n",
    "    print(pred)\n",
    "    print_scores(y_test,pred)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "def print_scores(labels, predicted_labels):\n",
    "    print(\"Accuracy: {}\".format(accuracy_score(labels, predicted_labels)))\n",
    "    print(\"Precision: {}\".format(precision_score(labels, predicted_labels)))\n",
    "    print(\"Recall: {}\".format(recall_score(labels, predicted_labels)))\n",
    "    print(\"F1: {}\".format(f1_score(labels, predicted_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
